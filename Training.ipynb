{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import optim\n",
    "from hydra.experimental import compose, initialize\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNNSM.rnnsm import RNNSM\n",
    "from RMTPP.rmtpp import RMTPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATMDataset(Dataset):\n",
    "    def __init__(self, path, max_len=500, normalization='none'):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        data = pd.read_csv(path)\n",
    "        self.ids = data['id']\n",
    "        self.times = data['time']\n",
    "        self.events = data['event'] + 1\n",
    "        self.time_seqs, self.event_seqs = self.generate_sequence()\n",
    "        \n",
    "            \n",
    "\n",
    "    def generate_sequence(self):\n",
    "        time_seqs = []\n",
    "        event_seqs = []\n",
    "        cur_start, cur_end = 0, 1\n",
    "        \n",
    "        # нарезка датасета по последовательностям с одним id и максимальной длиной self.max_len\n",
    "        while cur_start < len(self.ids):\n",
    "            if cur_end < len(self.ids) and \\\n",
    "                    self.ids[cur_start] == self.ids[cur_end] and \\\n",
    "                    cur_end - cur_start < self.max_len:\n",
    "                cur_end += 1\n",
    "                continue\n",
    "            else:\n",
    "                time_seqs.append(torch.Tensor(self.times[cur_start:cur_end].to_numpy()))\n",
    "                event_seqs.append(torch.LongTensor(self.events[cur_start:cur_end].to_numpy()))\n",
    "                cur_start, cur_end = cur_end, cur_end + 1\n",
    "\n",
    "        return time_seqs, event_seqs\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.time_seqs[item], self.event_seqs[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_seqs)\n",
    "\n",
    "    \n",
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad[..., None], yy_pad[..., None], x_lens, y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ATMDataset('data/ATM/train_day.csv')\n",
    "data_loader = DataLoader(dataset=ds, batch_size=32, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize(config_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rmtpp(data_loader, model, cfg: DictConfig):\n",
    "    train_metrics = defaultdict(list)\n",
    "\n",
    "    for epoch in range(cfg.training.n_epochs):\n",
    "        print(f'Epoch {epoch+1}/{cfg.training.n_epochs}....')\n",
    "        \n",
    "        for times, events, lengths, _ in data_loader:\n",
    "            time_deltas = torch.cat([torch.zeros(times.shape[0], 1, times.shape[2]),\n",
    "                                     times[:, 1:] - times[:, :-1]],\n",
    "                                     dim=1)\n",
    "            o_t, y_t = model(events, time_deltas, lengths)\n",
    "            padding_mask = torch.isclose(times, torch.Tensor([0]))\n",
    "            ret_mask = ~padding_mask\n",
    "            loss = model.compute_loss(time_deltas, padding_mask, o_t, y_t[0], events)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_metrics['loss'].append(loss.item())\n",
    "            #print(loss)\n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3, 3)\n",
    "a.flatten(0, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5....\n",
      "Time prediction loss: -3154.877197265625\n",
      "Markers loss: 946.3025512695312\n",
      "f_deltas.min(): 2.1765725219093985e-16\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: 366.3349914550781\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n",
      "Time prediction loss: nan\n",
      "Markers loss: nan\n",
      "f_deltas.min(): nan\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-deb1db297f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMTPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_rmtpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-7791c21451cb>\u001b[0m in \u001b[0;36mtrain_rmtpp\u001b[0;34m(data_loader, model, cfg)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             time_deltas = torch.cat([torch.zeros(times.shape[0], 1, times.shape[2]),\n\u001b[0;32m----> 9\u001b[0;31m                                      times[:, 1:] - times[:, :-1]],\n\u001b[0m\u001b[1;32m     10\u001b[0m                                      dim=1)\n\u001b[1;32m     11\u001b[0m             \u001b[0mo_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RMTPP(cfg.rmtpp)\n",
    "opt = optim.Adam(model.parameters(), lr=cfg.training.lr)\n",
    "train_rmtpp(data_loader, model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnnsm(data_loader, model, cfg: DictConfig):\n",
    "    train_metrics = defaultdict(list)\n",
    "\n",
    "    for epoch in range(cfg.training.n_epochs):\n",
    "        print(f'Epoch {epoch+1}/{cfg.training.n_epochs}....')\n",
    "        \n",
    "        for times, events, lengths, _ in data_loader:\n",
    "            time_deltas = torch.cat([torch.zeros(times.shape[0], 1, times.shape[2]),\n",
    "                                     times[:, 1:] - times[:, :-1]],\n",
    "                                     dim=1)\n",
    "            o_t = model(events, time_deltas, lengths)\n",
    "            print(o_t.shape)\n",
    "            padding_mask = torch.isclose(times, torch.Tensor([0]))\n",
    "            ret_mask = ~padding_mask\n",
    "            loss = model.compute_loss(time_deltas, padding_mask, ret_mask, o_t)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_metrics['loss'].append(loss.item())\n",
    "            print(loss)\n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5....\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(1115.8245, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(509.7157, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(-365.4796, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(-707.6506, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(-3218.4873, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(-2914.9414, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(-3494.3152, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n",
      "tensor(-5074.1763, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 1]) torch.Size([32, 500]) torch.Size([32, 500, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-4489639afcc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNSM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnnsm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_rnnsm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-4f1b55bd5dcc>\u001b[0m in \u001b[0;36mtrain_rnnsm\u001b[0;34m(data_loader, model, cfg)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mret_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RNNSM(cfg.rnnsm)\n",
    "opt = optim.Adam(model.parameters(), lr=cfg.training.lr)\n",
    "train_rnnsm(data_loader, model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
