{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Recurrent Neural Network Survival Model:Predicting Web User Return Time, Grob et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem \n",
    "\n",
    "The problem solved in Grob et. al. is similar to that of Du et. al., meaning that input is a sequence of time/marker pairs, however, the end goal is to predict not only the return time of a user, but also whether they will return during the prediction window or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics\n",
    "\n",
    "Mathematics of Grob et. al. is almost identical to that of Du et. al., so look it up in RMTPP folder. The only differences are the likelihood function and the prediction formula on inference. The log-likelihood function authors use here is $\\mathcal{l}(\\mathcal{C}) = \\Sigma_{i}\\Sigma_{j}\\mathcal{l}(t_{j+1}^i)$,\n",
    "where $\\mathcal{l}(t_{j+1}^i) = \\log f^*(t_{j+1}^i)$ if $i$'th user did not return during the prediction window and $j$ is the last even for $i$'th user, and $\\mathcal{l}(t_{j+1}^i) = \\log S^*(t_{j+1}^i)$ otherwise. The functions $f^*$ and $S^*$ are exactly the same as in Du et. al.'s work.\n",
    "\n",
    "On inference, they predict time between the last session and the next one as $\\widehat{d}_{j+1} = \\mathbb{E}[\\mathcal{T}|\\mathcal{H_{j}}] = \\int_{t_{j}}^{\\infty}S^*(t)dt$, where the integral is computed numerically. However, this expression allows the model to predict return time before the start of prediction window, that is why they modify it as $$\\mathbb{E}[\\mathcal{T}|\\mathcal{T} > t_{s}] = \\frac{\\int_{t_s}^{\\infty}S^*(t)dt}{S(t_{s})} + \\int_{0}^{t_{s}}S^*(t)dt$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "Idea is also similar to Du et. al.'s work. They also use LSTM model with the same input structure: different markers are embedded and then combined via a dense layer. They also use LSTM's hidden state to calculate the value of $\\lambda^*(t_{j+1})$. The difference is that they do not have an auxilliary loss related to marker prediction, and their loss (negative of log-likelihood function mentioned above) incorporates cases of people who did not return within the prediction window."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
